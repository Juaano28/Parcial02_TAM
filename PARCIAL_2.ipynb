{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juaano28/Parcial02_TAM/blob/main/PARCIAL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parcial 2 TAM\n",
        "\n",
        "\n",
        "*   Nicolás Castaño Pérez\n",
        "*   Juan Esteban López Marin\n",
        "\n"
      ],
      "metadata": {
        "id": "dz6gPNQWhdfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n",
        "!pip install streamlit -q #instalación de librerías\n",
        "!pip install pyngrok\n",
        "!pip install optuna\n",
        "!pip install streamlit pandas matplotlib seaborn scikit-learn pyngrok kagglehub\n",
        "!pip install pyngrok streamlit --quiet"
      ],
      "metadata": {
        "id": "eZuj01R3hbKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EyadtKAZ8Tgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVDh_rLjFcMB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "with h5py.File('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/usps.h5', 'r') as hf:\n",
        "        train = hf.get('train')\n",
        "        X_tr = train.get('data')[:]\n",
        "        y_tr = train.get('target')[:]\n",
        "        test = hf.get('test')\n",
        "        X_te = test.get('data')[:]\n",
        "        y_te = test.get('target')[:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Punto (b) – Proyección USPS con PCA y UMAP\n",
        "\n",
        "###  Objetivo\n",
        "Este análisis busca proyectar el conjunto de datos USPS a un espacio bidimensional usando dos técnicas de reducción de dimensionalidad: **PCA** (Análisis de Componentes Principales) y **UMAP** (Uniform Manifold Approximation and Projection). Además, se explora cómo varía la representación obtenida al modificar el parámetro `n_neighbors` en UMAP.\n",
        "\n",
        "---\n",
        "\n",
        "###  Comparación entre PCA y UMAP\n",
        "\n",
        "- **PCA** es un método lineal que proyecta los datos en las direcciones que maximizan la varianza global. En la proyección obtenida:\n",
        "  - Se observa cierta agrupación por dígito.\n",
        "  - Sin embargo, hay bastante **solapamiento entre clases**, especialmente entre dígitos similares como el 3 y el 8.\n",
        "  - La estructura de los datos no se representa bien cuando hay no linealidades o múltiples variedades locales.\n",
        "\n",
        "- **UMAP**, en cambio, es un método no lineal que busca preservar tanto la **estructura local** como la **estructura global** del conjunto de datos:\n",
        "  - Produce **clústeres más compactos y claramente separados**.\n",
        "  - El agrupamiento se alinea mejor con las etiquetas reales de los dígitos.\n",
        "  - Se logra una representación más rica de la estructura latente.\n",
        "\n",
        "---\n",
        "\n",
        "###  Efecto del parámetro `n_neighbors` en UMAP\n",
        "\n",
        "Se exploraron cuatro valores: **5, 15, 50, 100**.\n",
        "\n",
        "- **n_neighbors = 5**:\n",
        "  - Alta preservación local, los clústeres son muy compactos.\n",
        "  - Riesgo de fragmentación (más ruido visual).\n",
        "\n",
        "- **n_neighbors = 15**:\n",
        "  - Equilibrio entre estructura local y global.\n",
        "  - Clústeres bien definidos con suficiente separación.\n",
        "\n",
        "- **n_neighbors = 50**:\n",
        "  - Los clústeres tienden a ser más grandes y más difusos.\n",
        "  - Más coherencia global, pero menos detalle local.\n",
        "\n",
        "- **n_neighbors = 100**:\n",
        "  - Predomina la estructura global, se pierde separación entre clases similares.\n",
        "  - Posible fusión de clases cercanas.\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusión\n",
        "\n",
        "UMAP ofrece una proyección **más adecuada para visualización y análisis exploratorio** en comparación con PCA, especialmente cuando se requiere preservar relaciones no lineales entre muestras. Además, el parámetro `n_neighbors` es crucial: valores bajos priorizan detalles locales, mientras que valores altos promueven una estructura más global pero menos precisa por clase.\n"
      ],
      "metadata": {
        "id": "wrd3GjrL6w7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "\n",
        "# Verifica que X_tr y y_tr estén definidos previamente\n",
        "# Ejemplo (si tienes los datos guardados):\n",
        "# X_tr = np.load(\"usps_X.npy\")\n",
        "# y_tr = np.load(\"usps_y.npy\")\n",
        "\n",
        "# Mostrar dimensiones del conjunto de datos\n",
        "print(f\"USPS (HDF5): {X_tr.shape[0]} muestras, {X_tr.shape[1]} dimensiones\")\n",
        "\n",
        "# Asegurar que las etiquetas sean enteros\n",
        "y_tr = y_tr.astype(int)\n",
        "\n",
        "# --------------------------\n",
        "# Proyección PCA a 2D\n",
        "# --------------------------\n",
        "pca = PCA(n_components=2, random_state=0)\n",
        "X_pca = pca.fit_transform(X_tr)\n",
        "\n",
        "# --------------------------\n",
        "# Proyección UMAP a 2D con configuración base\n",
        "# --------------------------\n",
        "umap_model = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric=\"euclidean\",\n",
        "    random_state=0\n",
        ")\n",
        "X_umap = umap_model.fit_transform(X_tr)\n",
        "\n",
        "# --------------------------\n",
        "# Función de visualización con imágenes superpuestas\n",
        "# --------------------------\n",
        "def plot_embedding(X_emb, title, show_images=True, sample_images=100, ax=None):\n",
        "    if ax is None:\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        ax = plt.gca()\n",
        "\n",
        "    # Crear un DataFrame para visualización\n",
        "    df = pd.DataFrame({\n",
        "        \"x\": X_emb[:, 0],\n",
        "        \"y\": X_emb[:, 1],\n",
        "        \"label\": y_tr\n",
        "    })\n",
        "\n",
        "    # Graficar los puntos coloreados por etiqueta\n",
        "    sns.scatterplot(\n",
        "        data=df,\n",
        "        x=\"x\", y=\"y\",\n",
        "        hue=\"label\",\n",
        "        palette=\"tab10\",\n",
        "        s=5, alpha=0.6,\n",
        "        legend=\"full\",\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    ax.legend(title=\"Dígito\", loc='best')\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Superponer imágenes representativas (opcional)\n",
        "    if show_images:\n",
        "        indices = np.random.choice(len(X_tr), size=sample_images, replace=False)\n",
        "        for idx in indices:\n",
        "            xi, yi = X_emb[idx]\n",
        "            img = X_tr[idx].reshape(16, 16)\n",
        "            ax.imshow(\n",
        "                img, cmap=\"gray\",\n",
        "                extent=(xi-0.5, xi+0.5, yi-0.5, yi+0.5),\n",
        "                alpha=0.7\n",
        "            )\n",
        "\n",
        "# --------------------------\n",
        "# Visualización PCA vs UMAP base\n",
        "# --------------------------\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# PCA\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "plot_embedding(X_pca, \"Proyección PCA 2D – USPS\", ax=ax1)\n",
        "\n",
        "# UMAP (n_neighbors=15)\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "plot_embedding(X_umap, \"Proyección UMAP 2D (n_neighbors=15) – USPS\", ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# Comparación UMAP con diferentes n_neighbors\n",
        "# --------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "neighbors = [5, 15, 50, 100]\n",
        "\n",
        "for i, nn in enumerate(neighbors):\n",
        "    # Proyectar con UMAP para cada valor de n_neighbors\n",
        "    emb = umap.UMAP(\n",
        "        n_components=2,\n",
        "        n_neighbors=nn,\n",
        "        min_dist=0.1,\n",
        "        metric=\"euclidean\",\n",
        "        random_state=0\n",
        "    ).fit_transform(X_tr)\n",
        "\n",
        "    # Visualizar sin superposición de imágenes\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    plot_embedding(emb, f\"UMAP n_neighbors={nn}\", show_images=False, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2LN4XuYK6xrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C)"
      ],
      "metadata": {
        "id": "ZYaCZ0TV8zUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Punto (c) – Clasificación con 3 modelos\n",
        "\n",
        "Se evaluaron tres modelos de clasificación sobre tres espacios distintos de representación: el conjunto **original** de datos USPS, y sus proyecciones usando **PCA (20 componentes)** y **UMAP (20 dimensiones)**.\n",
        "\n",
        "Los modelos elegidos son:\n",
        "\n",
        "---\n",
        "\n",
        "###  1. Regresión Logística (Logistic Regression)\n",
        "\n",
        "- **Justificación**:\n",
        "  - Es un modelo lineal probabilístico muy eficiente para clasificación multiclase.\n",
        "  - Sirve como línea base robusta para comparar otros modelos.\n",
        "  - Su interpretación es clara y no requiere mucha afinación.\n",
        "\n",
        "- **Hiperparámetros usados**:\n",
        "  - `solver='lbfgs'`: optimizador eficiente para clasificación multiclase.\n",
        "  - `multi_class='multinomial'`: modela directamente la probabilidad multiclase (mejor que one-vs-rest).\n",
        "  - `max_iter=1000`: asegura convergencia con datos proyectados de alta dimensionalidad.\n",
        "\n",
        "- **Observaciones**:\n",
        "  - Se desempeñó de forma razonable, especialmente en datos proyectados por PCA.\n",
        "  - Sensible a relaciones no lineales, lo que limita su rendimiento en UMAP o datos originales.\n",
        "\n",
        "---\n",
        "\n",
        "###  2. Random Forest Classifier\n",
        "\n",
        "- **Justificación**:\n",
        "  - Modelo robusto y no paramétrico basado en árboles.\n",
        "  - Captura relaciones no lineales y es poco sensible a outliers.\n",
        "  - Ofrece alta precisión sin requerir normalización de datos.\n",
        "\n",
        "- **Hiperparámetros usados**:\n",
        "  - `n_estimators=100`: 100 árboles para garantizar estabilidad.\n",
        "  - `max_depth=20`: limita la profundidad para evitar sobreajuste.\n",
        "\n",
        "- **Observaciones**:\n",
        "  - Buen desempeño tanto en el espacio original como en UMAP.\n",
        "  - Aprovecha la variabilidad de la representación para mejorar la clasificación.\n",
        "  - Más lento que la regresión logística, pero más preciso en estructuras complejas.\n",
        "\n",
        "---\n",
        "\n",
        "###  3. Red Neuronal Profunda (MLP)\n",
        "\n",
        "- **Justificación**:\n",
        "  - Permite modelar relaciones complejas y no lineales entre las variables.\n",
        "  - Capaz de adaptarse a representaciones densas como UMAP o PCA.\n",
        "  - Permite obtener probabilidades suaves para cada clase → útil para ROC.\n",
        "\n",
        "- **Arquitectura**:\n",
        "  - `Dense(128, relu) → Dropout(0.3) → Dense(64, relu) → Dense(10, softmax)`\n",
        "  - Dos capas ocultas con activación ReLU y una capa de salida softmax para clasificación multiclase.\n",
        "\n",
        "- **Hiperparámetros usados**:\n",
        "  - `epochs=20`: entrenamiento corto para evitar sobreajuste.\n",
        "  - `batch_size=64`: tamaño de lote equilibrado.\n",
        "  - `Dropout(0.3)`: regularización que previene overfitting.\n",
        "  - `optimizer=Adam(learning_rate=0.001)`: adaptativo y eficiente para tareas multicategoría.\n",
        "\n",
        "- **Observaciones**:\n",
        "  - Modelo más costoso en tiempo de entrenamiento.\n",
        "  - Excelente rendimiento sobre datos proyectados con UMAP, al capturar mejor las relaciones complejas.\n",
        "\n",
        "---\n",
        "\n",
        "###  Evaluación\n",
        "\n",
        "- Se utilizó `train_test_split` con 20% de test para todos los modelos.\n",
        "- Se midió `accuracy`, `classification_report` y se graficó la **curva ROC multiclase** para cada clasificador.\n",
        "- Las curvas ROC permiten comparar la capacidad discriminativa del modelo en cada clase.\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusión\n",
        "\n",
        "Los tres modelos permiten evaluar desde soluciones lineales simples hasta clasificadores no lineales y complejos. Los resultados muestran que:\n",
        "- **PCA** mejora ligeramente el desempeño de modelos lineales.\n",
        "- **UMAP** ofrece mejores resultados para modelos como Random Forest y redes neuronales.\n",
        "- La **red neuronal** sobresale en espacios no lineales y ofrece predicciones más suaves y calibradas.\n"
      ],
      "metadata": {
        "id": "3dunlUrQw9q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import h5py\n",
        "\n",
        "# Cargar datos USPS\n",
        "with h5py.File('/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/usps.h5', 'r') as hf:\n",
        "    X = hf['train']['data'][:]\n",
        "    y = hf['train']['target'][:].astype(int)\n",
        "\n",
        "# Proyecciones\n",
        "X_pca = PCA(n_components=20).fit_transform(X)\n",
        "X_umap = umap.UMAP(n_components=20, n_neighbors=15, random_state=0).fit_transform(X)\n",
        "\n",
        "# Datos para clasificación\n",
        "datasets = {\n",
        "    \"Original\": X,\n",
        "    \"PCA (20)\": X_pca,\n",
        "    \"UMAP (20)\": X_umap\n",
        "}\n",
        "\n",
        "# Clasificadores clásicos\n",
        "def evaluate_classifier(clf, X_train, X_test, y_train, y_test, title):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"=== {title} ===\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # ROC multiclase\n",
        "    y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "    y_score = clf.predict_proba(X_test)\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    for i in range(10):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i in range(10):\n",
        "        plt.plot(fpr[i], tpr[i], label=f\"Clase {i} (AUC = {roc_auc[i]:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title(f\"Curva ROC - {title}\")\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Clasificador simple basado en Deep Learning\n",
        "def evaluate_nn(X, y, title):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    y_train_cat = to_categorical(y_train, 10)\n",
        "    y_test_cat = to_categorical(y_test, 10)\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(X_train, y_train_cat, validation_split=0.1, epochs=20, batch_size=64, verbose=0)\n",
        "\n",
        "    loss, acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
        "    print(f\"=== NN - {title} ===\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # ROC multiclase\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(10):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_cat[:, i], y_pred_prob[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i in range(10):\n",
        "        plt.plot(fpr[i], tpr[i], label=f\"Clase {i} (AUC = {roc_auc[i]:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title(f\"Curva ROC - NN {title}\")\n",
        "    plt.xlabel(\"FPR\")\n",
        "    plt.ylabel(\"TPR\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Evaluar cada conjunto proyectado\n",
        "for name, X_proj in datasets.items():\n",
        "    print(f\"\\n\\n--- Evaluación: {name} ---\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_proj, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
        "    evaluate_classifier(lr, X_train, X_test, y_train, y_test, f\"LogReg - {name}\")\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=0)\n",
        "    evaluate_classifier(rf, X_train, X_test, y_train, y_test, f\"RF - {name}\")\n",
        "\n",
        "    # Red neuronal simple\n",
        "    evaluate_nn(X_proj, y, f\"{name}\")\n"
      ],
      "metadata": {
        "id": "5n3vuBKs81Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d)"
      ],
      "metadata": {
        "id": "O5mmuFL0axVI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a798fe"
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "\n",
        "# Define the directory to save the models\n",
        "model_dir = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/trained_models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Load data explicitly\n",
        "data_path = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/usps.h5'\n",
        "with h5py.File(data_path, 'r') as hf:\n",
        "    train = hf.get('train')\n",
        "    X_tr = train.get('data')[:]\n",
        "    y_tr = train.get('target')[:]\n",
        "    test = hf.get('test')\n",
        "    X_te = test.get('data')[:]\n",
        "    y_te = test.get('target')[:]\n",
        "X = np.vstack((X_tr, X_te))\n",
        "y = np.hstack((y_tr, y_te)).astype(int)\n",
        "\n",
        "\n",
        "# Perform Dimensionality Reduction (using 20 components for classification as in point c)\n",
        "pca_20d = PCA(n_components=20, random_state=0)\n",
        "X_pca_20d = pca_20d.fit_transform(X)\n",
        "\n",
        "umap_20d = umap.UMAP(n_components=20, n_neighbors=15, min_dist=0.1, metric=\"euclidean\", random_state=0)\n",
        "X_umap_20d = umap_20d.fit_transform(X)\n",
        "\n",
        "# Create datasets dictionary\n",
        "datasets_to_save = {\n",
        "    \"Original\": X,\n",
        "    \"PCA_20\": X_pca_20d,\n",
        "    \"UMAP_20\": X_umap_20d # Corrected variable name here\n",
        "}\n",
        "\n",
        "# Train and save models\n",
        "for name, X_data in datasets_to_save.items():\n",
        "    print(f\"Training models on {name} dataset...\")\n",
        "    # Split data for training (use the same split as in the dashboard for consistency)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    lr_model = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    lr_filename = os.path.join(model_dir, f'lr_model_{name}.pickle')\n",
        "    with open(lr_filename, 'wb') as f:\n",
        "        pickle.dump(lr_model, f)\n",
        "    print(f\"Saved Logistic Regression model for {name} to {lr_filename}\")\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=0)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    rf_filename = os.path.join(model_dir, f'rf_model_{name}.pickle')\n",
        "    with open(rf_filename, 'wb') as f:\n",
        "        pickle.dump(rf_model, f)\n",
        "    print(f\"Saved Random Forest model for {name} to {rf_filename}\")\n",
        "\n",
        "    # Train Neural Network\n",
        "    # Need to adjust input shape for the NN\n",
        "    input_dim = X_train.shape[1]\n",
        "    nn_model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    nn_model.fit(X_train, to_categorical(y_train, 10), validation_split=0.1, epochs=20, batch_size=64, verbose=0) # Train NN\n",
        "\n",
        "    nn_filename = os.path.join(model_dir, f'nn_model_{name}.h5') # Save NN model in HDF5 format\n",
        "    nn_model.save(nn_filename)\n",
        "    print(f\"Saved Neural Network model for {name} to {nn_filename}\")\n",
        "\n",
        "print(\"\\nAll models trained and saved successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "934a236c"
      },
      "source": [
        "%%writefile usps_dashboard.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import umap\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential, load_model # Import load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import h5py\n",
        "import io\n",
        "import pickle # Import pickle\n",
        "import os.path # Explicitly import os.path\n",
        "\n",
        "# Define the directory where models are saved\n",
        "model_dir = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/trained_models'\n",
        "\n",
        "\n",
        "# Título del Dashboard\n",
        "st.title(\"Análisis de Datos USPS con Reducción de Dimensionalidad y Clasificación\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "Este dashboard interactivo permite explorar la proyección del conjunto de datos de dígitos escritos a mano USPS utilizando\n",
        "técnicas de reducción de dimensionalidad como PCA y UMAP. Además, evalúa el rendimiento de diferentes clasificadores\n",
        "(Regresión Logística, Random Forest y Red Neuronal) en los datos originales y en los espacios de menor dimensión.\n",
        "\n",
        "Utilice la barra lateral para configurar los parámetros de visualización y clasificación.\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# Cargar datos\n",
        "@st.cache_data\n",
        "def load_usps_data(filepath):\n",
        "    \"\"\"Loads the USPS dataset from an HDF5 file.\"\"\"\n",
        "    with h5py.File(filepath, 'r') as hf:\n",
        "        train = hf.get('train')\n",
        "        X_tr = train.get('data')[:]\n",
        "        y_tr = train.get('target')[:]\n",
        "        test = hf.get('test')\n",
        "        X_te = test.get('data')[:]\n",
        "        y_te = test.get('target')[:]\n",
        "    return X_tr, y_tr, X_te, y_te\n",
        "\n",
        "data_path = '/content/drive/Shareddrives/UNAL_Colab/Teoría de Aprendizaje de Máquina/Parcial_2/usps.h5'\n",
        "X_tr, y_tr, X_te, y_te = load_usps_data(data_path)\n",
        "X = np.vstack((X_tr, X_te))\n",
        "y = np.hstack((y_tr, y_te))\n",
        "\n",
        "y = y.astype(int) # Ensure labels are integers\n",
        "\n",
        "# --- Dimensionality Reduction Functions ---\n",
        "@st.cache_data\n",
        "def perform_pca(X, n_components):\n",
        "    \"\"\"Performs Principal Component Analysis (PCA) on the data.\"\"\"\n",
        "    st.info(f\"Aplicando PCA con {n_components} componentes.\")\n",
        "    pca = PCA(n_components=n_components, random_state=0)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "\n",
        "@st.cache_data\n",
        "def perform_umap(X, n_components, n_neighbors, min_dist, metric):\n",
        "    \"\"\"Performs Uniform Manifold Approximation and Projection (UMAP) on the data.\"\"\"\n",
        "    st.info(f\"Aplicando UMAP con {n_components} componentes, n_neighbors={n_neighbors}, min_dist={min_dist}, metric='{metric}'.\")\n",
        "    umap_model = umap.UMAP(\n",
        "        n_components=n_components,\n",
        "        n_neighbors=n_neighbors, # Controls local vs global structure\n",
        "        min_dist=min_dist,       # Controls how tightly points are clustered together\n",
        "        metric=metric,           # Distance metric\n",
        "        random_state=0\n",
        "    )\n",
        "    X_umap = umap_model.fit_transform(X)\n",
        "    return X_umap\n",
        "\n",
        "# --- Visualization Function ---\n",
        "def plot_embedding(X_emb, y, title, show_images=False, sample_images=100, X_original=None):\n",
        "    \"\"\"Plots the 2D data embedding, optionally with superimposed images.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"x\": X_emb[:, 0],\n",
        "        \"y\": X_emb[:, 1],\n",
        "        \"label\": y\n",
        "    })\n",
        "\n",
        "    sns.scatterplot(\n",
        "        data=df,\n",
        "        x=\"x\", y=\"y\",\n",
        "        hue=\"label\",\n",
        "        palette=\"tab10\", # A good default categorical palette\n",
        "        s=10, alpha=0.7, # Point size and transparency\n",
        "        legend=\"full\",\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(title)\n",
        "    ax.legend(title=\"Dígito\", loc='best')\n",
        "    ax.grid(True)\n",
        "\n",
        "    # Superponer imágenes representativas (opcional) solo si es 2D\n",
        "    if show_images and X_original is not None and X_emb.shape[1] == 2:\n",
        "        # Use a fixed random state for reproducibility in image sampling\n",
        "        np.random.seed(0)\n",
        "        indices = np.random.choice(len(X_original), size=sample_images, replace=False)\n",
        "        for idx in indices:\n",
        "            xi, yi = X_emb[idx]\n",
        "            img = X_original[idx].reshape(16, 16) # Reshape the 256 features back to 16x16 image\n",
        "\n",
        "            # Adjust extent based on your data scale if needed for better visual\n",
        "            # The extent should match the approximate scale of your projected points.\n",
        "            # This is a heuristic, might need adjustment based on actual projection ranges.\n",
        "            # Calculate rough scale of the embedding for image sizing\n",
        "            x_range = np.max(X_emb[:, 0]) - np.min(X_emb[:, 0])\n",
        "            y_range = np.max(X_emb[:, 1]) - np.min(X_emb[:, 1])\n",
        "            img_size_factor = 0.015 # Adjust this factor to change image size relative to plot size\n",
        "\n",
        "            ax.imshow(\n",
        "                img, cmap=\"gray\", # Grayscale colormap for digits\n",
        "                extent=(xi - x_range * img_size_factor, xi + x_range * img_size_factor,\n",
        "                        yi - y_range * img_size_factor, yi + y_range * img_size_factor),\n",
        "                alpha=0.8, # Slightly more opaque images\n",
        "                aspect='auto', # Allow aspect ratio to be determined by extent\n",
        "                interpolation='bilinear' # Smooth out pixelated images\n",
        "            )\n",
        "\n",
        "\n",
        "    st.pyplot(fig)\n",
        "    plt.close(fig) # Close the figure to free memory\n",
        "\n",
        "def plot_roc_curve(fpr_dict, tpr_dict, roc_auc_dict, title):\n",
        "    \"\"\"Plots the multi-class ROC curve.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    for i in range(10):\n",
        "        if i in fpr_dict and fpr_dict[i] is not None: # Check if class exists and data is not None\n",
        "            ax.plot(fpr_dict[i], tpr_dict[i], label=f\"Clase {i} (AUC = {roc_auc_dict[i]:.2f})\")\n",
        "        else:\n",
        "             # Plot a minimal line or skip if no data for this class\n",
        "             ax.plot([], [], label=f\"Clase {i} (No data)\") # Add empty plot for missing classes in legend\n",
        "\n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Aleatorio (AUC = 0.50)') # Add diagonal line for random chance\n",
        "    ax.set_title(f\"Curva ROC - {title}\")\n",
        "    ax.set_xlabel(\"Tasa de Falsos Positivos (FPR)\")\n",
        "    ax.set_ylabel(\"Tasa de Verdaderos Positivos (TPR)\")\n",
        "    ax.legend(loc='lower right') # Position legend\n",
        "    ax.grid(True)\n",
        "    st.pyplot(fig)\n",
        "    plt.close(fig) # Close the figure to free memory\n",
        "\n",
        "\n",
        "# --- Classification Functions ---\n",
        "# Load models instead of evaluating classifiers\n",
        "@st.cache_resource # Use cache_resource for models as they are large objects\n",
        "def load_classifier_model(model_name, dataset_name):\n",
        "    \"\"\"Loads a pre-trained scikit-learn classifier model.\"\"\"\n",
        "    model_path = os.path.join(model_dir, f'{model_name}_model_{dataset_name}.pickle')\n",
        "    if os.path.exists(model_path):\n",
        "        st.info(f\"Cargando modelo: {model_name} en dataset: {dataset_name}\")\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        return model\n",
        "    st.warning(f\"Modelo {model_name} para {dataset_name} no encontrado en {model_path}\")\n",
        "    return None\n",
        "\n",
        "@st.cache_resource # Use cache_resource for models\n",
        "def load_nn_model(dataset_name):\n",
        "    \"\"\"Loads a pre-trained Keras Neural Network model.\"\"\"\n",
        "    model_path = os.path.join(model_dir, f'nn_model_{dataset_name}.h5')\n",
        "    if os.path.exists(model_path):\n",
        "        st.info(f\"Cargando modelo: Neural Network en dataset: {dataset_name}\")\n",
        "        model = load_model(model_path)\n",
        "        return model\n",
        "    st.warning(f\"Modelo Neural Network para {dataset_name} no encontrado en {model_path}\")\n",
        "    return None\n",
        "\n",
        "# Function to get classification report and ROC data from loaded models\n",
        "def get_classification_metrics(model, X_test, y_test, y_all_classes):\n",
        "    \"\"\"Calculates classification metrics and ROC data for a given model.\"\"\"\n",
        "    if model is None:\n",
        "        return None, None, {}, {}, {} # Return empty data if model is None\n",
        "\n",
        "    # Get predicted class labels\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_prob = model.predict_proba(X_test)\n",
        "        y_pred = model.predict(X_test) # Use predict for scikit-learn models\n",
        "    elif hasattr(model, 'predict'): # For Neural Networks\n",
        "        y_pred_prob = model.predict(X_test)\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1) # Get class with highest probability for Keras models\n",
        "    else:\n",
        "        st.error(\"Model does not have 'predict_proba' or 'predict' method.\")\n",
        "        return None, None, {}, {}, {}\n",
        "\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Convert report to DataFrame for better display in Streamlit\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "    report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "\n",
        "    # ROC multiclase\n",
        "    y_test_bin = label_binarize(y_test, classes=np.unique(y_all_classes))\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    if y_pred_prob is not None:\n",
        "        n_classes = len(np.unique(y_all_classes))\n",
        "        # Ensure y_score has the correct shape (n_samples, n_classes)\n",
        "        if y_pred_prob.shape[1] == n_classes:\n",
        "            for i in np.unique(y_all_classes):\n",
        "                if i in np.unique(y_test):\n",
        "                    try:\n",
        "                         # Ensure y_test_bin[:, i] is for the current class and y_score[:, i] are probabilities\n",
        "                         fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])\n",
        "                         roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "                    except Exception as e:\n",
        "                         #st.warning(f\"Could not calculate ROC for class {i}: {e}\") # Avoid excessive warnings in UI\n",
        "                         fpr[i], tpr[i], roc_auc[i] = None, None, None # Assign None if calculation fails\n",
        "                else:\n",
        "                    # Class not present in the test set, cannot calculate ROC\n",
        "                    fpr[i], tpr[i], roc_auc[i] = None, None, None # Assign None if class not in test set\n",
        "        else:\n",
        "            st.warning(f\"Mismatch in y_score shape ({y_pred_prob.shape}) and number of classes ({n_classes}). Cannot calculate ROC.\")\n",
        "            # Ensure empty ROC data is returned\n",
        "            for i in np.unique(y_all_classes):\n",
        "                 fpr[i], tpr[i], roc_auc[i] = None, None, None\n",
        "\n",
        "\n",
        "    return acc, report_df, fpr, tpr, roc_auc\n",
        "\n",
        "\n",
        "# --- Streamlit App Layout ---\n",
        "st.sidebar.header(\"Configuración\")\n",
        "analysis_mode = st.sidebar.radio(\"Seleccione el Modo de Análisis\", [\"Proyección 2D\", \"Clasificación\"])\n",
        "\n",
        "if analysis_mode == \"Proyección 2D\":\n",
        "    st.header(\"Proyección a 2D (Visualización)\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "    Seleccione una técnica de reducción de dimensionalidad para visualizar el conjunto de datos USPS en 2 dimensiones.\n",
        "    Esto ayuda a entender la estructura y separabilidad de las clases.\n",
        "    \"\"\")\n",
        "\n",
        "    projection_method = st.sidebar.selectbox(\"Método de Proyección 2D\", [\"PCA\", \"UMAP\"])\n",
        "\n",
        "    if projection_method == \"PCA\":\n",
        "        st.subheader(\"Proyección PCA a 2D\")\n",
        "        st.markdown(\"\"\"\n",
        "        **PCA (Análisis de Componentes Principales)** es un método lineal que proyecta los datos\n",
        "        en las direcciones de máxima varianza. Es útil para visualizar las diferencias globales\n",
        "        entre clases.\n",
        "        \"\"\")\n",
        "        X_pca_2d = perform_pca(X, n_components=2)\n",
        "        plot_embedding(X_pca_2d, y, \"Proyección PCA 2D – USPS\", show_images=True, X_original=X)\n",
        "\n",
        "    elif projection_method == \"UMAP\":\n",
        "        st.subheader(\"Proyección UMAP a 2D\")\n",
        "        st.markdown(\"\"\"\n",
        "        **UMAP (Uniform Manifold Approximation and Projection)** es un método no lineal que\n",
        "        busca preservar tanto la estructura local como la global. A menudo produce clústeres\n",
        "        más compactos y separados que PCA, siendo ideal para visualizaciones donde las clases\n",
        "        tienen fronteras no lineales.\n",
        "        \"\"\")\n",
        "\n",
        "        st.sidebar.subheader(\"Parámetros de UMAP 2D\")\n",
        "        n_neighbors_umap = st.sidebar.slider(\n",
        "            \"n_neighbors\",\n",
        "            5, 100, 15,\n",
        "            help=\"Número de vecinos a considerar para construir el gráfico del co-vecindario. Valores bajos (<10) enfatizan la estructura local; valores altos (>50) enfatizan la estructura global.\"\n",
        "        )\n",
        "        min_dist_umap = st.sidebar.slider(\n",
        "            \"min_dist\",\n",
        "            0.0, 1.0, 0.1, 0.05,\n",
        "            help=\"Controla la distancia mínima entre puntos en el espacio de baja dimensión. Valores bajos resultan en clústeres más compactos; valores altos resultan en una dispersión más laxa.\"\n",
        "        )\n",
        "        metric_umap = st.sidebar.selectbox(\n",
        "            \"Metric\",\n",
        "            [\"euclidean\", \"manhattan\", \"cosine\"],\n",
        "            help=\"Métrica de distancia para construir el gráfico del co-vecindario en el espacio original.\"\n",
        "        )\n",
        "\n",
        "\n",
        "        X_umap_2d = perform_umap(X, n_components=2, n_neighbors=n_neighbors_umap, min_dist=min_dist_umap, metric=metric_umap)\n",
        "        plot_embedding(X_umap_2d, y, f\"Proyección UMAP 2D (n_neighbors={n_neighbors_umap}, min_dist={min_dist_umap}) – USPS\", show_images=True, X_original=X)\n",
        "\n",
        "        st.subheader(\"Comparación UMAP con diferentes n_neighbors\")\n",
        "        st.markdown(\"\"\"\n",
        "        Esta sección muestra cómo la proyección UMAP varía al cambiar el parámetro `n_neighbors`.\n",
        "        Observe cómo valores bajos tienden a \"fragmentar\" la visualización (priorizando la estructura local),\n",
        "        mientras que valores altos pueden fusionar clústeres (priorizando la estructura global).\n",
        "        \"\"\")\n",
        "        neighbors_compare = [5, 15, 50, 100]\n",
        "        cols = st.columns(2) # Use columns for better layout\n",
        "\n",
        "        for i, nn in enumerate(neighbors_compare):\n",
        "            col = cols[i % 2]\n",
        "            with col:\n",
        "                st.write(f\"**n_neighbors = {nn}**\")\n",
        "                # Add description based on n_neighbors value\n",
        "                if nn == 5:\n",
        "                    st.write(\"_Enfatiza la estructura local, clústeres muy compactos, posible fragmentación._\")\n",
        "                elif nn == 15:\n",
        "                     st.write(\"_Equilibrio entre estructura local y global, clústeres bien definidos._\")\n",
        "                elif nn == 50:\n",
        "                     st.write(\"_Más coherencia global, clústeres más difusos, menos detalle local._\")\n",
        "                elif nn == 100:\n",
        "                     st.write(\"_Predomina la estructura global, posible fusión de clases similares._\")\n",
        "\n",
        "                X_umap_nn = perform_umap(X, n_components=2, n_neighbors=nn, min_dist=0.1, metric=\"euclidean\")\n",
        "                plot_embedding(X_umap_nn, y, f\"UMAP n_neighbors={nn}\", show_images=False) # No images in comparison plots\n",
        "\n",
        "\n",
        "elif analysis_mode == \"Clasificación\":\n",
        "    st.header(\"Evaluación de Clasificadores\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "    Esta sección evalúa el rendimiento de diferentes modelos de clasificación\n",
        "    en el conjunto de datos original y en las proyecciones obtenidas por PCA y UMAP.\n",
        "    Los modelos fueron pre-entrenados y cargados para su evaluación.\n",
        "    \"\"\")\n",
        "\n",
        "    st.sidebar.subheader(\"Configuración de Clasificación\")\n",
        "    n_components_dr = st.sidebar.slider(\n",
        "        \"Componentes para DR (Clasificación)\",\n",
        "        2, 256, 20,\n",
        "        help=\"Número de componentes a mantener después de aplicar PCA o UMAP para la clasificación.\"\n",
        "    )\n",
        "    classifier_method = st.sidebar.selectbox(\n",
        "        \"Clasificador\",\n",
        "        [\"Logistic Regression\", \"Random Forest\", \"Neural Network\"],\n",
        "        help=\"Seleccione el modelo de clasificación a evaluar.\"\n",
        "    )\n",
        "\n",
        "    st.subheader(f\"Resultados de Clasificación con {classifier_method} (n_components={n_components_dr})\")\n",
        "\n",
        "    # Perform DR for classification (need to ensure consistency with how models were trained)\n",
        "    # We need to apply the same DR to the full dataset and then split for test\n",
        "    # Add information about the DR being performed for classification\n",
        "    st.info(f\"Aplicando Reducción de Dimensionalidad ({n_components_dr} componentes) a los datos completos antes de la división test/train para la evaluación.\")\n",
        "\n",
        "    X_pca_dr_full = perform_pca(X, n_components=n_components_dr)\n",
        "    # Using default UMAP params for DR for classification, consistent with training cell\n",
        "    X_umap_dr_full = perform_umap(X, n_components=n_components_dr, n_neighbors=15, min_dist=0.1, metric=\"euclidean\")\n",
        "\n",
        "    datasets_for_classification = {\n",
        "        \"Original\": X,\n",
        "        f\"PCA ({n_components_dr})\": X_pca_dr_full,\n",
        "        f\"UMAP ({n_components_dr})\": X_umap_dr_full\n",
        "    }\n",
        "\n",
        "    classification_results_dict = {} # Store results for display\n",
        "    roc_data = {} # Store ROC data for plotting\n",
        "\n",
        "    # Map classifier method string to the prefix used in roc_data keys (consistent with evaluation cell)\n",
        "    classifier_prefix_map = {\n",
        "        \"Logistic Regression\": \"LogReg\",\n",
        "        \"Random Forest\": \"RF\",\n",
        "        \"Neural Network\": \"NN\"\n",
        "    }\n",
        "    # Get the actual name used for saving models based on the selected classifier\n",
        "    model_file_prefix_map = {\n",
        "        \"Logistic Regression\": \"lr\",\n",
        "        \"Random Forest\": \"rf\",\n",
        "        \"Neural Network\": \"nn\"\n",
        "    }\n",
        "    selected_model_prefix = model_file_prefix_map.get(classifier_method, \"\")\n",
        "\n",
        "\n",
        "    for name, X_data in datasets_for_classification.items():\n",
        "        st.write(f\"### Dataset: {name}\")\n",
        "        # Split data for evaluation (using the same random state as training)\n",
        "        # This ensures we evaluate on the exact same test set used during training\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Load the appropriate model based on the selected classifier and dataset name\n",
        "        if classifier_method == \"Neural Network\":\n",
        "             # For NN, the dataset name used in the filename is slightly different (e.g., \"UMAP_20\")\n",
        "             # We need to map the display name to the filename name\n",
        "             filename_dataset_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") # Convert \"PCA (20)\" to \"PCA_20\" etc.\n",
        "             model = load_nn_model(filename_dataset_name)\n",
        "        else:\n",
        "             # For LR and RF, the dataset name in the filename is also adjusted (e.g., \"PCA_20\")\n",
        "             filename_dataset_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "             model = load_classifier_model(selected_model_prefix, filename_dataset_name)\n",
        "\n",
        "\n",
        "        if model:\n",
        "            st.subheader(f\"Métricas de {classifier_method} en {name}\")\n",
        "            acc, report_df, fpr, tpr, roc_auc = get_classification_metrics(model, X_test, y_test, y)\n",
        "\n",
        "            if acc is not None:\n",
        "                st.write(f\"**Accuracy:** {acc:.4f}\")\n",
        "\n",
        "            if report_df is not None:\n",
        "                st.write(\"**Reporte de Clasificación:**\")\n",
        "                st.dataframe(report_df) # Display report as a table\n",
        "\n",
        "            # Store ROC data with a consistent key structure for plotting\n",
        "            # Use the display name for clarity in the ROC plot title\n",
        "            roc_data[f\"{classifier_method} - {name}\"] = {\"fpr\": fpr, \"tpr\": tpr, \"roc_auc\": roc_auc}\n",
        "        else:\n",
        "            st.warning(f\"No se pudo cargar el modelo {classifier_method} para el dataset {name}.\")\n",
        "            # Ensure empty ROC data is stored if model not found to avoid errors later\n",
        "            roc_data[f\"{classifier_method} - {name}\"] = {\"fpr\": {}, \"tpr\": {}, \"roc_auc\": {}}\n",
        "\n",
        "\n",
        "    # Plot ROC curves for the selected classifier across different datasets\n",
        "    st.subheader(\"Curvas ROC Comparativas\")\n",
        "    st.markdown(f\"\"\"\n",
        "    Estas curvas ROC muestran el rendimiento de la **{classifier_method}**\n",
        "    para cada clase en los diferentes conjuntos de datos (Original, PCA, UMAP).\n",
        "    Una curva más cercana a la esquina superior izquierda indica un mejor rendimiento.\n",
        "    El área bajo la curva (AUC) resume la capacidad discriminativa del clasificador\n",
        "    para esa clase (1.0 es perfecto, 0.5 es aleatorio).\n",
        "    \"\"\")\n",
        "\n",
        "    # Filter roc_data based on the selected classifier name (using the display name for filtering)\n",
        "    filtered_roc_data = {key: value for key, value in roc_data.items() if key.startswith(classifier_method)}\n",
        "\n",
        "\n",
        "    if filtered_roc_data:\n",
        "        # Check if there is any valid ROC data to plot after filtering\n",
        "        # A more robust check: check if any of the filtered entries have non-empty fpr dictionaries\n",
        "        has_valid_roc_data_to_plot = any(data.get(\"fpr\") and any(v is not None for v in data[\"fpr\"].values()) for key, data in filtered_roc_data.items())\n",
        "\n",
        "        if has_valid_roc_data_to_plot:\n",
        "             # Sort keys for consistent plotting order (e.g., Original, PCA, UMAP)\n",
        "             # This custom sort places 'Original' first, then 'PCA', then 'UMAP'\n",
        "             def custom_sort_key(item):\n",
        "                 key = item[0]\n",
        "                 if \"Original\" in key:\n",
        "                     return 0\n",
        "                 elif \"PCA\" in key:\n",
        "                     return 1\n",
        "                 elif \"UMAP\" in key:\n",
        "                     return 2\n",
        "                 else:\n",
        "                     return 3 # Other keys come last\n",
        "\n",
        "             sorted_filtered_items = sorted(filtered_roc_data.items(), key=custom_sort_key)\n",
        "\n",
        "             for key, data in sorted_filtered_items:\n",
        "                 if data.get(\"fpr\") and any(v is not None for v in data[\"fpr\"].values()):\n",
        "                     plot_roc_curve(data[\"fpr\"], data[\"tpr\"], data[\"roc_auc\"], key) # Use the key as the title\n",
        "                 else:\n",
        "                     st.info(f\"No hay datos completos de Curva ROC disponibles para {key}.\")\n",
        "\n",
        "        else:\n",
        "             st.info(f\"No hay datos de Curva ROC disponibles para el clasificador {classifier_method} en los conjuntos de datos seleccionados.\")\n",
        "\n",
        "    else:\n",
        "        st.info(\"No hay datos de Curva ROC disponibles para el clasificador y conjuntos de datos seleccionados.\")\n",
        "\n",
        "    st.subheader(\"Hiperparámetros Utilizados y Justificación (Clasificadores)\")\n",
        "    st.markdown(\"\"\"\n",
        "    Los modelos de clasificación utilizados en este dashboard fueron entrenados con los siguientes hiperparámetros,\n",
        "    seleccionados para ofrecer un buen balance entre rendimiento y eficiencia:\n",
        "\n",
        "    *   **Regresión Logística (`LogisticRegression`)**:\n",
        "        *   `max_iter=1000`: Aumentado a 1000 para asegurar que el modelo converja, especialmente en datos con mayor dimensionalidad o complejidad como las proyecciones.\n",
        "        *   `solver='lbfgs'`: Optimizador predeterminado y eficiente para conjuntos de datos pequeños y medianos. Funciona bien para clasificación multiclase.\n",
        "        *   `multi_class='multinomial'`: Configurado para manejar directamente la clasificación multiclase (más de 2 clases) en lugar de usar una estrategia \"one-vs-rest\".\n",
        "\n",
        "    *   **Random Forest (`RandomForestClassifier`)**:\n",
        "        *   `n_estimators=100`: Número de árboles en el bosque. Un valor de 100 es un buen punto de partida que generalmente proporciona estabilidad sin un costo computacional excesivo. Más árboles tienden a mejorar el rendimiento hasta cierto punto.\n",
        "        *   `max_depth=20`: Profundidad máxima de cada árbol. Limitar la profundidad ayuda a prevenir el sobreajuste a los datos de entrenamiento, manteniendo la capacidad de generalización.\n",
        "        *   `random_state=0`: Asegura la reproducibilidad de los resultados.\n",
        "\n",
        "    *   **Red Neuronal Profunda (MLP - construida con Keras)**:\n",
        "        *   **Arquitectura**:\n",
        "            *   Capa de entrada: Define la forma de los datos de entrada (igual al número de características del dataset: 256 para Original, 20 para PCA/UMAP).\n",
        "            *   `Dense(128, activation='relu')`: Primera capa oculta con 128 neuronas y función de activación ReLU (Rectified Linear Unit), que introduce no linealidad.\n",
        "            *   `Dropout(0.3)`: Técnica de regularización que desactiva aleatoriamente el 30% de las neuronas durante el entrenamiento para reducir el sobreajuste.\n",
        "            *   `Dense(64, activation='relu')`: Segunda capa oculta con 64 neuronas y activación ReLU. Permite al modelo aprender representaciones más complejas.\n",
        "            *   `Dense(10, activation='softmax')`: Capa de salida con 10 neuronas (una por cada dígito/clase) y función de activación Softmax, que produce una distribución de probabilidad sobre las 10 clases.\n",
        "        *   **Configuración de Entrenamiento**:\n",
        "            *   `optimizer=Adam(learning_rate=0.001)`: Algoritmo de optimización adaptativo que ajusta la tasa de aprendizaje durante el entrenamiento. Adam es una opción popular y eficiente. Se usa una tasa de aprendizaje inicial de 0.001.\n",
        "            *   `loss='categorical_crossentropy'`: Función de pérdida adecuada para problemas de clasificación multiclase con etiquetas codificadas como one-hot (usando `to_categorical`).\n",
        "            *   `metrics=['accuracy']`: Métrica para evaluar el rendimiento durante el entrenamiento y la evaluación.\n",
        "            *   `epochs=20`: Número de pasadas completas sobre el conjunto de entrenamiento. 20 épocas es un valor relativamente bajo, elegido para un entrenamiento más rápido en este ejemplo, pero podría aumentarse si se observara subajuste.\n",
        "            *   `batch_size=64`: Número de muestras por actualización de gradiente. Un tamaño de lote de 64 es común y equilibra la estabilidad del gradiente con la velocidad de entrenamiento.\n",
        "            *   `validation_split=0.1`: Una fracción (10%) del conjunto de entrenamiento se reserva para validar el modelo durante el entrenamiento y monitorear el sobreajuste.\n",
        "    \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "\n",
        "#Ejecutar Streamlit\n",
        "!streamlit run usps_dashboard.py &>/content/logs.txt & #Cambiar TAMExam1.py por el nombre de tu archivo principal\n",
        "\n",
        "#Exponer el puerto 8501 con Cloudflare Tunnel\n",
        "!cloudflared tunnel --url http://localhost:8501 > /content/cloudflared.log 2>&1 &\n",
        "\n",
        "#Leer la URL pública generada por Cloudflare\n",
        "import time\n",
        "time.sleep(5)  # Esperar que se genere la URL\n",
        "\n",
        "import re\n",
        "found_context = False  # Indicador para saber si estamos en la sección correcta\n",
        "\n",
        "with open('/content/cloudflared.log') as f:\n",
        "    for line in f:\n",
        "        #Detecta el inicio del contexto que nos interesa\n",
        "        if \"Your quick Tunnel has been created\" in line:\n",
        "            found_context = True\n",
        "\n",
        "        #Busca una URL si ya se encontró el contexto relevante\n",
        "        if found_context:\n",
        "            match = re.search(r'https?://\\S+', line)\n",
        "            if match:\n",
        "                url = match.group(0)  #Extrae la URL encontrada\n",
        "                print(f'Tu aplicación está disponible en: {url}')\n",
        "                break  #Termina el bucle después de encontrar la URL"
      ],
      "metadata": {
        "id": "XyzQ_zdcINWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "res = input(\"Digite (1) para finalizar la ejecución del Dashboard: \")\n",
        "\n",
        "if res.upper() == \"1\":\n",
        "    # Find the process ID (PID) of the streamlit process\n",
        "    # This assumes only one streamlit process is running\n",
        "    try:\n",
        "        # Use pgrep to find the PID of the streamlit process\n",
        "        pid = os.popen(\"pgrep -f streamlit\").read().strip()\n",
        "        if pid:\n",
        "            os.system(f\"kill {pid}\")  # Terminate the Streamlit process\n",
        "            print(f\"El proceso de Streamlit (PID: {pid}) ha sido finalizado.\")\n",
        "        else:\n",
        "            print(\"No se encontró ningún proceso de Streamlit ejecutándose.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al intentar finalizar el proceso de Streamlit: {e}\")\n",
        "\n",
        "    # Also try to stop the cloudflared tunnel process\n",
        "    try:\n",
        "        # Use pgrep to find the PID of the cloudflared process\n",
        "        pid_cf = os.popen(\"pgrep -f cloudflared\").read().strip()\n",
        "        if pid_cf:\n",
        "             # Use kill -9 for forceful termination\n",
        "            os.system(f\"kill -9 {pid_cf}\")  # Terminate the cloudflared process\n",
        "            print(f\"El proceso de Cloudflared (PID: {pid_cf}) ha sido finalizado.\")\n",
        "        else:\n",
        "            print(\"No se encontró ningún proceso de Cloudflared ejecutándose.\")\n",
        "    except Exception as e:\n",
        "         print(f\"Error al intentar finalizar el proceso de Cloudflared: {e}\")"
      ],
      "metadata": {
        "id": "t5AIhWeyIQqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 <PID>"
      ],
      "metadata": {
        "id": "pk_UeLahhBnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}